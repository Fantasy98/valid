{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mix of expert Sparse gate in Pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yuning/anaconda3/envs/tensor2/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "from torch import nn \n",
    "from torch.distributions.normal import Normal\n",
    "import numpy as np"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Define Experts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self,input_size,output_size,hidden_size) -> None:\n",
    "        super(MLP,self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size,hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size,output_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.softmax = nn.Softmax()\n",
    "    def forward(self,x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.relu(x)\n",
    "        return x\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SparseDispatcher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SparseDispatcher(object):\n",
    "    \"\"\"\n",
    "    A class to create input minibatches for the experts and to combine the results of experts to form a unified output\n",
    "    Two functions:\n",
    "        1. dispatch- take an input tensor and create input tensors for each expert\n",
    "        2.combine- take output tensors from each expert and form a combined output tensor. \n",
    "                    Outputs from different experts for the same batch element are summed together, weighted by the provided gates\n",
    "    \n",
    "    The class is initialized with a \"gates\" tensor, which specifies which batch elements go to which experts and the weights to use when combing the outputs\n",
    "    Batch element is sent to expert if gates[index_b,index_e] != 0 \n",
    "    The inputs and outputs are 2D , shape =(batch,depth)\n",
    "    \n",
    "    Caller is responsible for collapsing addtional dimensions prior to calling this class and reshaping the output to the original shpae\n",
    "    See common_layers.reshape_like()\n",
    "\n",
    "    An Example:\n",
    "    gates: a float 32 Tensor with shape (batch_size,num_experts)\n",
    "    inputs: a float 32 Tensor with shape (batch_size,input_size)\n",
    "    experts: a list of length of num_experts containing expert network\n",
    "\n",
    "    dispatcher = SpareDispatcher(num_experts,gates)\n",
    "    \n",
    "    expert_inputs = dispatcher.dispatch(input)\n",
    "    expert_outpts = [  experts[i](experts_inputs[i]) for i in range(num_experts)   ]\n",
    "    \n",
    "    outputs = dispatcher.combine(expert_outputs)\n",
    "\n",
    "    The preceding code sets the output for a particular example b to:\n",
    "    output[b] = Sum_i(gates[b,i]*experts[i](inputs[b]))\n",
    "\n",
    "    This class takes advantage of sparsity in the gate matrix by including in the Tensors for expert i only the batch elements for which gates [b,i]>0 \n",
    "    \"\"\"\n",
    "    def __init__(self,num_experts,gates) -> None:\n",
    "        self._gates = gates\n",
    "        self._num_experts = num_experts\n",
    "        # Sort experts which has non-zero weight\n",
    "        sorted_experts, index_sorted_experts = torch.nonzero(gates).sort(0)\n",
    "        #drop indices\n",
    "        _,self._expert_index = sorted_experts.split(split_size=1,dim=1) \n",
    "        # get accroding batch index for each expert \n",
    "        self._batch_index =torch.nonzero(gates)[index_sorted_experts[:,1],0]\n",
    "        # calculate number of samples that each expert get \n",
    "        self._part_sizes = (gates>0).sum(0).tolist()\n",
    "        # expand gates to match with self._batch_index\n",
    "        gates_exp = gates[self._batch_index.flatten()]\n",
    "        self._non_zero_gates = torch.gather(gates_exp,1,self._expert_index)\n",
    "    def dispatch(self,inp):\n",
    "        inp_exp = inp[self._batch_index].squeeze(1)\n",
    "        return torch.split(inp_exp,self._part_sizes,dim=0)\n",
    "    \n",
    "    def combine(self,expert_out,multiply_by_gates= True):\n",
    "\n",
    "        # apply exp to expert outputs so that we are not longer in log space\n",
    "        stitched = torch.cat(expert_out,0).exp()\n",
    "        if multiply_by_gates:\n",
    "            stitched = stitched.mul(self._non_zero_gates)\n",
    "        zeros = torch.zeros(self._gates.size(0),expert_out[-1].size(1),requires_grad=True,device=stitched.device)\n",
    "        combined = zeros.index_add(0,self._batch_index,stitched.float())\n",
    "        # add eps to all zero values in order to avoid nan when going back\n",
    "        combined[combined==0] = np.finfo(float).eps\n",
    "        return combined.log()\n",
    "    \n",
    "    def expert_to_gates(self):\n",
    "        # Split nonzero gates for each expert\n",
    "        return torch.split(self._non_zero_gates,self._part_sizes,dim=0)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 MoE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MoE(nn.Module):\n",
    "    def __init__(self,input_size,output_size,num_experts,hidden_size,noisy_gating=True,k = 4) -> None:\n",
    "        \"\"\"\n",
    "        The Module of MoE\n",
    "        Args:\n",
    "        input_size: size of input\n",
    "        output_size: size of output\n",
    "        num_experts: number of experts to be trained\n",
    "        hidden_size : the size of hidden layer\n",
    "        noisy_gate: boolean, if add noisy towards the input \n",
    "        k: int number of expert to be use for decision , should be <= num_experts\n",
    "        \"\"\"\n",
    "        super(MoE,self).__init__()\n",
    "        self.noisy_gating = noisy_gating\n",
    "        self.num_experts = num_experts\n",
    "        self.output_size = output_size\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.k = k \n",
    "\n",
    "        # Create experts with number of experts\n",
    "        self.experts = nn.ModuleList([MLP(self.input_size,self.output_size,self.hidden_size) for i in range(self.num_experts)])\n",
    "\n",
    "        # Gate , zero initialize the weight of gate \n",
    "        self.w_gate = nn.Parameter(torch.zeros(self.input_size,self.num_experts),requires_grad=True)\n",
    "        self.w_noise = nn.Parameter(torch.zeros(self.input_size,self.num_experts),requires_grad=True)\n",
    "\n",
    "        self.softplus =nn.Softplus()\n",
    "        self.softmax = nn.Softmax(dim = 1)\n",
    "        self.register_buffer(\"mean\",torch.tensor([0.0]))\n",
    "        self.register_buffer(\"std\",torch.tensor([1.0]))\n",
    "\n",
    "        assert(self.k <= self.num_experts)\n",
    "\n",
    "    def cv_squared(self,x):\n",
    "        \"\"\"\n",
    "        compute squared coeff of variation of a sample\n",
    "        Useful as a loss to encourage a positive distribution to be more uniform\n",
    "        \"\"\"\n",
    "        eps = 1e-10\n",
    "        # if only 1 expert\n",
    "        if x.shape[0] == 1:\n",
    "            return torch.tensor([0],device=x.device,dtype=x.dtype)\n",
    "        return x.float().var() / (x.float().mean()**2 + eps)\n",
    "    \n",
    "\n",
    "    def _gates_to_load(self,gates):\n",
    "        \"\"\"\n",
    "        Compute the true load per expert, given the gates\n",
    "        \"\"\"\n",
    "        return (gates > 0).sum(0)\n",
    "\n",
    "\n",
    "    def _prob_in_top_k(self,clean_values,noisy_values,\n",
    "                        noise_stddev,noisy_top_values):\n",
    "        \"\"\"\n",
    "        Computes the probability that value is in topk, given different random noise.\n",
    "        It is a way to backprop from a loss that balance the number \n",
    "        In the case of no noise, pass None ==> noise_stddev\n",
    "        Args:\n",
    "        clean_values: Tensor, size = (batch,n)\n",
    "        noisy_values: Tensor, size = (batch,n)\n",
    "                        Equal to clean_values + normaliy distributed noise with std = noise_stddev\n",
    "        noise_stddev: Tensor, size = (batch,n)\n",
    "        noisy_top_values: Tensor, size = (batch,m), \n",
    "                            values ouput of top_k(noisy_top_values,m), where m>= k+1\n",
    "\n",
    "        Returns:\n",
    "        A Tensor of shape (batch,n) \n",
    "        \"\"\"\n",
    "        batch = clean_values.size(0)\n",
    "        m = noisy_top_values.size(1)\n",
    "        top_values_flat = noisy_top_values.flatten()\n",
    "\n",
    "        threshold_positions_if_in = torch.arange(batch,device=clean_values.device)*m +self.k\n",
    "        threshold_if_in = torch.unsqueeze(torch.gather(top_values_flat,0,threshold_positions_if_in),1)\n",
    "        is_in = torch.gt(noisy_values,threshold_if_in) # A boolean tensor to check if the noisy value meet threshold\n",
    "        \n",
    "        threshold_positions_if_out = threshold_positions_if_in -1 \n",
    "        threshold_if_out = torch.unsqueeze(torch.gather(top_values_flat,0,threshold_positions_if_out),1)\n",
    "\n",
    "        normal  = Normal(self.mean,self.std)\n",
    "        prob_if_in = normal.cdf((clean_values- threshold_if_in)/noise_stddev)\n",
    "        prob_if_out = normal.cdf((clean_values- threshold_if_out)/noise_stddev)\n",
    "        prob = torch.where(is_in,prob_if_in,prob_if_out)\n",
    "        return prob\n",
    "\n",
    "\n",
    "    def noisy_top_k_gating(self,x,train,noise_epsilon = 1e-2):\n",
    "        \"\"\"\n",
    "        Noise top-k gating \n",
    "        Args:\n",
    "            x: input tensor with shape [batch_size, input_size]\n",
    "            train: A boolean, add noise only for training\n",
    "            noise_epsilon: float\n",
    "        Returns:\n",
    "            gates: a Tensor with shape [batch_size,num_experts]\n",
    "            load: a Tensor with shape [num_experts]\n",
    "        \"\"\"\n",
    "        clean_logits =x @ self.w_gate\n",
    "        if self.noisy_gating and train:\n",
    "            raw_noise_stddev = x @ self.w_noise\n",
    "            noise_stddev = (self.softplus(raw_noise_stddev) + noise_epsilon)\n",
    "            noisy_logits = clean_logits + (torch.rand_like(clean_logits)*noise_stddev)\n",
    "            logits = noisy_logits\n",
    "        else:\n",
    "            logits = clean_logits\n",
    "        \n",
    "        # calculate topk +1 that will be need for the noisy gates\n",
    "        top_logits , top_indices = logits.topk( min(self.k+1, self.num_experts),dim=1 )\n",
    "        top_k_logits = top_logits[:,:self.k]\n",
    "        top_k_indices = top_indices[:,:self.k]\n",
    "        top_k_gates = self.softmax(top_k_logits)\n",
    "\n",
    "        zeros = torch.zeros_like(logits,requires_grad=True)\n",
    "        gates = zeros.scatter(1, top_k_indices,top_k_gates) # dim, index, src\n",
    "\n",
    "        if self.noisy_gating and self.k < self.num_experts and train:\n",
    "            load = self._prob_in_top_k(clean_logits,noisy_logits,noise_stddev).sum(0)\n",
    "        else:\n",
    "            load = self._gates_to_load(gates)\n",
    "\n",
    "        return gates,load\n",
    "    \n",
    "    def forward(self,x,loss_coef = 1e-2):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "        x: Tensor, shape = (batch_size, input_size)\n",
    "        train: boolean\n",
    "        loss_coef: a scalar- multiplier on load-balancing losses\n",
    "        \n",
    "        Returns:\n",
    "        y: a tensor with shape (batch_size, output_size)\n",
    "        extra_training_loss: scalar, extra loss encourage all experts to be approximately equally used across a batch\n",
    "    \n",
    "        \"\"\"\n",
    "        gates, load = self.noisy_top_k_gating(x,self.training)\n",
    "        \n",
    "        importance = gates.sum(0)\n",
    "\n",
    "        loss = self.cv_squared(importance) + self.cv_squared(load)\n",
    "        loss *= loss_coef\n",
    "\n",
    "        dispatcher= SparseDispatcher(self.num_experts,gates)\n",
    "        expert_inputs= dispatcher.dispatch(x)\n",
    "        gates = dispatcher.expert_to_gates()\n",
    "        expert_outputs = [ self.experts[i](expert_inputs[i])  for i in range(self.num_experts)   ]\n",
    "\n",
    "        y = dispatcher.combine(expert_outputs)\n",
    "        return y,loss\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensor2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15 (default, Nov 24 2022, 15:19:38) \n[GCC 11.2.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "11e05876bec2ad9b34c669d9dff61cc48fedec39522fd08af25791e3a216550b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
